{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg.special_matrices' (c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\scipy\\linalg\\special_matrices.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc2Vec, TaggedDocument\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtop2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Top2Vec\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.1.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial_matrices\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m triu\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n\u001b[0;32m     26\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg.special_matrices' (c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\scipy\\linalg\\special_matrices.py)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import praw\n",
    "import spacy\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from top2vec import Top2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up API keys (ensure security in a real application)\n",
    "NEWS_API_KEY = 'your_news_api_key'\n",
    "reddit = praw.Reddit(client_id='pC9dZB_wqZ9Son7HPJUc8w',\n",
    "                     client_secret='4hpR2C7Fpov0QGM7g5QNKJqsN0SmRA',\n",
    "                     user_agent='narative by /u/No_Type7179')\n",
    "\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def get_news(keyword, page_size=20):\n",
    "    url = f'https://newsapi.org/v2/everything?q={keyword}&apiKey={NEWS_API_KEY}&pageSize={page_size}'\n",
    "    response = requests.get(url)\n",
    "    articles = response.json().get('articles', [])\n",
    "    \n",
    "    news = []\n",
    "    for article in articles:\n",
    "        news.append({\n",
    "            'title': article['title'],\n",
    "            'description': article['description'],\n",
    "            'url': article['url'],\n",
    "            'publishedAt': article['publishedAt']\n",
    "        })\n",
    "    return news\n",
    "\n",
    "# Function to fetch top Reddit posts\n",
    "def fetch_top_posts(search_query, subreddit_name='all', limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    top_posts = subreddit.search(search_query, sort='top', limit=limit)\n",
    "    \n",
    "    post_data = []\n",
    "    for post in top_posts:\n",
    "        post_datetime = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        upvotes = post.ups\n",
    "        downvotes = upvotes - post.score\n",
    "        awards = post.all_awards if hasattr(post, 'all_awards') else []\n",
    "        award_names = [award['name'] for award in awards]\n",
    "        flair = post.link_flair_text if post.link_flair_text else \"No Flair\"\n",
    "        \n",
    "        post_data.append({\n",
    "            'Title': post.title,\n",
    "            'Body': post.selftext,\n",
    "            'Date': post_datetime,\n",
    "            'Upvotes': upvotes,\n",
    "            'Downvotes': downvotes,\n",
    "            'Comments': post.num_comments,\n",
    "            'Link': post.url,\n",
    "            'Awards': award_names,\n",
    "            'Flair': flair\n",
    "        })\n",
    "    \n",
    "    return post_data\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Named Entity Recognition\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Sentiment analysis with explicit model specification\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\", revision=\"af0f99b\")\n",
    "def analyze_sentiment(text):\n",
    "    return sentiment_analyzer(text)\n",
    "\n",
    "# Topic Modeling with Top2Vec\n",
    "def get_topics(documents):\n",
    "    # Prepare tagged data for Doc2Vec\n",
    "    tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(documents)]\n",
    "    \n",
    "    # Initialize and build vocabulary\n",
    "    model = Doc2Vec(vector_size=100, min_count=2, epochs=40, workers=4)\n",
    "    model.build_vocab(tagged_data)  # Ensure vocabulary is built\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    # Use Top2Vec for topic modeling\n",
    "    top2vec_model = Top2Vec(documents, embedding_model='universal-sentence-encoder')\n",
    "    \n",
    "    topics, topic_words, word_scores, topic_scores = top2vec_model.get_topics()\n",
    "    return topics, topic_words, word_scores, topic_scores\n",
    "\n",
    "# Event Timeline Plotting\n",
    "def plot_timeline(events):\n",
    "    event_dates = [datetime.strptime(event['date'], \"%Y-%m-%dT%H:%M:%SZ\") for event in events]\n",
    "    event_texts = [event['text'] for event in events]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(event_dates, [i for i in range(len(event_dates))], 'bo-')\n",
    "    \n",
    "    for i, text in enumerate(event_texts):\n",
    "        plt.text(event_dates[i], i, text, fontsize=9, verticalalignment='bottom', horizontalalignment='right')\n",
    "    \n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=2))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.title(\"Event Timeline\")\n",
    "    plt.xlabel(\"Date and Time\")\n",
    "    plt.ylabel(\"Events\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to combine everything\n",
    "def summarize_event(topic):\n",
    "    try:\n",
    "        # Fetch news and reddit data\n",
    "        news_articles = get_news(topic)\n",
    "        # reddit_posts = fetch_top_posts(topic)\n",
    "        \n",
    "        # Combine and preprocess data\n",
    "        combined_data = [article['description'] for article in news_articles if article['description']] \n",
    "        # + \\\n",
    "                        # [post['Body'] for post in reddit_posts if post['Body']]\n",
    "\n",
    "        cleaned_data = [preprocess_text(text) for text in combined_data]\n",
    "        \n",
    "        # Perform topic modeling\n",
    "        topics, topic_words, _, _ = get_topics(cleaned_data)\n",
    "        \n",
    "        # Extract entities and sentiment for each news and reddit post\n",
    "        events = []\n",
    "        for article in news_articles:\n",
    "            sentiment = analyze_sentiment(article['description'])\n",
    "            entities = extract_entities(article['description'])\n",
    "            events.append({\n",
    "                'text': article['title'],\n",
    "                'sentiment': sentiment,\n",
    "                'entities': entities,\n",
    "                'source': article['url'],\n",
    "                'date': article['publishedAt']\n",
    "            })\n",
    "        \n",
    "        for post in reddit_posts:\n",
    "            sentiment = analyze_sentiment(post['Body'])\n",
    "            entities = extract_entities(post['Body'])\n",
    "            events.append({\n",
    "                'text': post['Title'],\n",
    "                'sentiment': sentiment,\n",
    "                'entities': entities,\n",
    "                'source': post['Link'],\n",
    "                'date': post['Date']\n",
    "            })\n",
    "        \n",
    "        # Plot timeline\n",
    "        plot_timeline(events)\n",
    "        \n",
    "        # Print structured summary\n",
    "        print(\"Summary of Events:\")\n",
    "        for event in events:\n",
    "            print(f\"- {event['text']} (Source: {event['source']}, Date: {event['date']})\")\n",
    "            print(f\"  Sentiment: {event['sentiment'][0]['label']} ({event['sentiment'][0]['score']:.2f})\")\n",
    "            print(f\"  Key Entities: {', '.join([ent[0] for ent in event['entities']])}\")\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Input from user\n",
    "if __name__ == \"__main__\":\n",
    "    topic = input(\"Enter the event/topic: \")\n",
    "    summarize_event(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
