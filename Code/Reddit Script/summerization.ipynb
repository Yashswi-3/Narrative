{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import spacy\n",
    "from transformers import pipeline, DistilBertTokenizer\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Initialize the DistilBert tokenizer for text truncation\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Text Preprocessing: Tokenization, Lemmatization, and Stopword Removal\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Sentiment Analysis with text truncation\n",
    "def analyze_sentiment(text):\n",
    "    # Tokenize and truncate the text to the max length (512)\n",
    "    encoded_input = tokenizer(text, max_length=512, truncation=True, return_tensors='tf')\n",
    "\n",
    "    # Pass the encoded input to the sentiment analyzer\n",
    "    result = sentiment_analyzer(encoded_input['input_ids'])\n",
    "    \n",
    "    return result[0]['label'], result[0]['score']\n",
    "\n",
    "# Function to summarize events from the JSON input\n",
    "def summarize_events_from_json(json_filename):\n",
    "    try:\n",
    "        # Load JSON data\n",
    "        with open(json_filename, 'r') as f:\n",
    "            post_data = json.load(f)\n",
    "        \n",
    "        # Initialize an empty list to store summarized events\n",
    "        summarized_events = []\n",
    "\n",
    "        # Loop through each post in the JSON file\n",
    "        for post in post_data:\n",
    "            title = post['Title']\n",
    "            body = post['Body/Content']\n",
    "            link = post['Link']\n",
    "\n",
    "            # Combine title and body for processing\n",
    "            full_text = f\"{title}. {body}\" if body else title\n",
    "\n",
    "            # Preprocess the text\n",
    "            preprocessed_text = preprocess_text(full_text)\n",
    "\n",
    "            # Perform NER to extract entities\n",
    "            entities = extract_entities(full_text)\n",
    "\n",
    "            # Perform sentiment analysis\n",
    "            sentiment_label, sentiment_score = analyze_sentiment(full_text)\n",
    "\n",
    "            # Append the summarized event to the list\n",
    "            summarized_events.append({\n",
    "                'Title': title,\n",
    "                'Sentiment': f\"{sentiment_label} ({sentiment_score:.2f})\",\n",
    "                'Entities': entities,\n",
    "                'Link': link\n",
    "            })\n",
    "\n",
    "        # Print or return the summarized events\n",
    "        print(\"Summarized Events:\")\n",
    "        for event in summarized_events:\n",
    "            print(f\"- {event['Title']}\")\n",
    "            print(f\"  Sentiment: {event['Sentiment']}\")\n",
    "            print(f\"  Entities: {', '.join([f'{entity[0]} ({entity[1]})' for entity in event['Entities']])}\")\n",
    "            print(f\"  Link: {event['Link']}\")\n",
    "            print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Main function to take JSON filename as input and run the summarization\n",
    "if __name__ == \"__main__\":\n",
    "    json_filename = input(\"Enter the name of the JSON file: \")\n",
    "    summarize_events_from_json(json_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\yashswi\n",
      "[nltk_data]     shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\yashswi\n",
      "[nltk_data]     shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: TIL that a school in Poland is named after an Indian Maharaja, in honour of him providing shelter and free education to hundreds of Polish women and children after they had to escape Poland due to the World War 2.\n",
      "Link: http://www.thehindu.com/features/magazine/a-maharaja-in-warsaw/article3360283.ece\n",
      "Sentiment: Positive\n",
      "Entities: [('TIL', 'ORG'), ('Poland', 'GPE'), ('Indian', 'NORP'), ('hundreds', 'CARDINAL'), ('Polish', 'NORP'), ('Poland', 'GPE'), ('the World War 2', 'EVENT')]\n",
      "\n",
      "\n",
      "Title: List of the shit we've been through (so far) in 2020\n",
      "Link: https://www.reddit.com/r/teenagers/comments/iva8jo/list_of_the_shit_weve_been_through_so_far_in_2020/\n",
      "Sentiment: Positive\n",
      "Entities: [('2020', 'DATE'), ('Wikipedia', 'ORG'), ('2020', 'DATE'), ('June', 'DATE'), ('RIP Ruth Ginsberg', 'PERSON'), ('January 1st', 'DATE'), ('Annual', 'DATE'), ('Hong Kong', 'GPE'), ('more than one million', 'CARDINAL'), ('Indonesian', 'NORP'), ('Jakarta', 'GPE'), ('at least 66', 'CARDINAL'), ('about 60,000', 'CARDINAL'), ('January 2nd', 'DATE'), ('Australia', 'GPE'), ('bush', 'PERSON'), ('January 3rd', 'DATE'), ('U.S.', 'GPE'), ('Iranian', 'NORP'), ('Qasem Soleimani', 'PERSON'), ('January 7th', 'DATE'), ('Neil Peart', 'PERSON'), ('January 8th', 'DATE'), ('Iran', 'GPE'), ('2', 'CARDINAL'), ('Iraqi', 'NORP'), ('American', 'NORP'), ('Ukraine International Airlines', 'ORG'), ('Flight 752', 'PRODUCT'), ('176', 'CARDINAL'), ('January 12th', 'DATE'), ('Philippines', 'GPE'), ('39', 'CARDINAL'), ('January 16th', 'DATE'), ('Donald Trump', 'PERSON'), ('Senate', 'ORG'), ('January 21st', 'DATE'), ('Terry Jones', 'PERSON'), ('January 26th', 'DATE'), ('Kobe Bryant', 'PERSON'), ('January 28th', 'DATE'), ('7.7', 'CARDINAL'), ('Jamaica', 'GPE'), ('January 30th', 'DATE'), ('WHO', 'ORG'), ('COVID-19 a Public Health Emergency of International Concern\\n', 'ORG'), ('January 31st', 'DATE'), ('UK', 'GPE'), ('EU', 'ORG'), ('February', 'DATE'), ('February 5th', 'DATE'), ('The US Senate', 'ORG'), ('Donald Trump', 'PERSON'), ('February 11th', 'DATE'), ('WHO', 'ORG'), ('COVID-19', 'PERSON'), ('February 27th', 'DATE'), ('DOW', 'ORG'), ('1,190.95', 'DATE'), ('the worst week', 'DATE'), ('2008', 'DATE'), ('COVID-19', 'ORG'), ('February 28th', 'DATE'), ('Freeman Dyson', 'PERSON'), ('February 29th', 'DATE'), ('US', 'GPE'), ('Taliban', 'ORG'), ('US', 'GPE'), ('March 10th', 'DATE'), ('Afghanistan', 'GPE'), ('October 2001', 'DATE'), ('March 8th', 'DATE'), ('Italy', 'GPE'), ('more than a quarter', 'DATE'), ('A day later', 'DATE'), ('Italy', 'GPE'), ('first', 'ORDINAL'), ('March 9th', 'DATE'), ('Russo', 'NORP'), ('COVID-19', 'ORG'), ('DOW', 'ORG'), ('2000', 'CARDINAL'), ('as much as 30%', 'PERCENT'), ('1991', 'DATE'), ('March 11th', 'DATE'), ('WHO', 'ORG'), ('COVID-19', 'PERSON'), ('March 12th', 'DATE'), ('COVID-19', 'ORG'), ('US', 'GPE'), ('the Schengen Area', 'LOC'), ('DOW', 'ORG'), ('more than 2300', 'CARDINAL'), ('at least 30 days', 'DATE'), ('March 14th', 'DATE'), ('Spain', 'GPE'), ('COVID-19', 'ORG'), ('March 16th', 'DATE'), ('DOW', 'ORG'), ('2997.1', 'CARDINAL'), ('12.93%', 'PERCENT'), ('second', 'ORDINAL'), ('March 17th', 'DATE'), ('EU', 'ORG'), ('Schengen', 'PERSON'), ('at least 30 days', 'DATE'), ('COVID-19', 'ORG'), ('the Euro 2020', 'LAW'), ('2020', 'CARDINAL'), ('Copa America', 'LOC'), ('summer', 'DATE'), ('2021', 'DATE'), ('March 18th', 'DATE'), ('Eurovision 2020', 'DATE'), ('March 20th', 'DATE'), ('COVID-19', 'ORG'), ('10,000', 'CARDINAL'), ('250,000', 'CARDINAL'), ('March 24th', 'DATE'), ('India', 'GPE'), ('UK', 'GPE'), ('COVID-19', 'PERSON'), ('more than a third', 'CARDINAL'), ('The Summer Olympics', 'EVENT'), ('August 8th 2021', 'DATE'), ('China', 'GPE'), ('two days later', 'DATE'), ('March 26th', 'DATE'), ('half a million', 'CARDINAL'), ('23,000', 'CARDINAL'), ('US', 'GPE'), ('about 80,000', 'CARDINAL'), ('more than 1,000', 'CARDINAL'), ('Syria', 'GPE'), ('Yemen', 'GPE'), ('Libya', 'GPE'), ('March 30th', 'DATE'), ('9%', 'PERCENT'), ('23', 'MONEY'), ('November 2002', 'DATE'), ('April 2nd', 'DATE'), ('1 million', 'CARDINAL'), ('April 6th', 'DATE'), ('Trump', 'PERSON'), ('the Russian Imperial Movement', 'ORG'), ('April 7th', 'DATE'), ('Japan', 'GPE'), ('COVID', 'ORG'), ('¥108 trillion', 'MONEY'), ('$990 billion', 'MONEY'), ('20%', 'PERCENT'), ('April 8th', 'DATE'), ('Rick May', 'PERSON'), ('Saudi Arabia', 'GPE'), ('Yemen', 'GPE'), ('China', 'GPE'), ('Wuhan', 'GPE'), ('April 10th', 'DATE'), ('COVID', 'ORG'), ('100,000', 'CARDINAL'), ('EU', 'ORG'), ('€540 billion', 'MONEY'), ('April 14th', 'DATE'), ('Donald Trump', 'PERSON'), ('US', 'GPE'), ('WHO', 'ORG'), ('COVID', 'ORG'), ('China', 'GPE'), ('April 15th', 'DATE'), ('COVID', 'ORG'), ('2 million', 'CARDINAL'), ('the Tour de France', 'ORG'), ('April 17th', 'DATE'), ('Europe', 'LOC'), ('100,000', 'CARDINAL'), ('COVID-19', 'ORG'), ('April 19th', 'DATE'), ('Truro', 'GPE'), ('Canada', 'GPE'), ('at least 17', 'CARDINAL'), ('Iran', 'GPE'), ('the Persian Gulf', 'LOC'), ('Iranian', 'NORP'), ('US', 'GPE'), ('Paris', 'GPE'), ('Berlin', 'GPE'), ('Vladikavkaz', 'ORG'), ('April 20th', 'DATE'), ('West Texas Intermediate', 'PRODUCT'), ('South Korean', 'NORP'), ('Kim Jong-Un', 'PERSON'), ('April 25th', 'DATE'), ('200,000', 'CARDINAL'), ('UK', 'GPE'), ('20,000', 'CARDINAL'), ('April 27th', 'DATE'), ('Pentagon', 'ORG'), ('surpasses 3 million', 'CARDINAL'), ('US', 'GPE'), ('1 million', 'CARDINAL'), ('April 28th', 'DATE'), ('Lebanon', 'GPE'), ('the second day', 'DATE'), ('April 30th', 'DATE'), ('NASA', 'ORG'), ('American', 'NORP'), ('2024', 'DATE'), ('May 3rd-4th', 'DATE'), ('Silvercorp', 'ORG'), ('Venezuela', 'GPE'), ('May 5th', 'DATE'), ('UK', 'GPE'), ('Europe', 'LOC'), ('about 32,000', 'CARDINAL'), ('May 9th', 'DATE'), ('Indian', 'NORP'), ('May 10th', 'DATE'), ('Iran', 'GPE'), ('19', 'CARDINAL'), ('COVID', 'ORG'), ('4 million', 'CARDINAL'), ('May 14th', 'DATE'), ('300,000', 'CARDINAL'), ('UN', 'ORG'), ('May 15th', 'DATE'), ('The Last Airbender', 'ORG'), ('Netflix', 'GPE'), ('May 21st', 'DATE'), ('COVID-19', 'ORG'), ('5 million', 'CARDINAL'), ('May 22nd', 'DATE'), ('Brazil', 'GPE'), ('Russia', 'GPE'), ('second', 'ORDINAL'), ('over 330,000', 'CARDINAL'), ('May 24th', 'DATE'), ('Western Australia', 'ORG'), ('a decade', 'DATE'), ('May 26th', 'DATE'), ('hundreds', 'CARDINAL'), ('American', 'NORP'), ('George Floyd', 'PERSON'), ('May 27th', 'DATE'), ('China', 'GPE'), ('Congress', 'ORG'), ('Hong Kong', 'GPE'), ('US', 'GPE'), ('State', 'ORG'), ('Hong Kong', 'GPE'), ('US', 'GPE'), ('100,000', 'CARDINAL'), ('Americans', 'NORP'), ('116,000', 'CARDINAL'), ('US', 'GPE'), ('May 30th', 'DATE'), ('Cape Canaveral', 'GPE'), ('Florida', 'GPE'), ('first', 'ORDINAL'), ('American', 'NORP'), ('2011', 'DATE'), ('May 31st', 'DATE'), ('COVID', 'ORG'), ('6 million', 'CARDINAL'), ('June 1st', 'DATE'), ('Trump', 'ORG'), ('Antifa', 'PERSON'), ('Lafayette Square', 'FAC'), ('June 3rd', 'DATE'), ('Boris Johnson', 'PERSON'), ('UK', 'GPE'), ('Hong Kong', 'GPE'), ('UK', 'GPE'), ('China', 'GPE'), ('Putin', 'PERSON'), ('the Ambarnaya River', 'LOC'), ('June 4th', 'DATE'), ('Libya', 'GPE'), ('Government of National Accord', 'ORG'), ('June 6th', 'DATE'), ('June 7th', 'DATE'), ('400,000', 'CARDINAL'), ('June 8th', 'DATE'), ('COVID', 'ORG'), ('7 million', 'CARDINAL'), ('Seattle', 'GPE'), ('June 9th', 'DATE'), ('Harvard', 'ORG'), ('COVID', 'ORG'), ('China', 'GPE'), ('August 2019', 'DATE'), ('June 15th', 'DATE'), ('20', 'CARDINAL'), ('Indian', 'NORP'), ('over 40', 'CARDINAL'), ('Chinese', 'NORP'), ('Turkey', 'GPE'), ('Iran', 'GPE'), (\"the Kurdistan Worker's Party\", 'ORG'), ('Iraqi', 'NORP'), ('Kurdistan', 'GPE'), ('June 16th', 'DATE'), ('COVID-19', 'ORG'), ('8 million', 'CARDINAL'), ('North Korea', 'GPE'), ('the Inter-Korean Liaison Office', 'ORG'), ('South Korea', 'GPE'), ('Kaesong', 'GPE'), ('June 22nd', 'DATE'), ('COVID-19', 'ORG'), ('9 million', 'CARDINAL'), ('June 23rd', 'DATE'), ('7.5', 'CARDINAL'), ('Mexico', 'GPE'), ('Yemeni', 'NORP'), ('13', 'CARDINAL'), ('Saudi', 'NORP'), ('2', 'CARDINAL'), ('June 27th', 'DATE'), ('Micheál Martin', 'PERSON'), ('Taoiseach', 'PERSON'), ('Ireland', 'GPE'), ('June 28th', 'DATE'), ('COVID-19', 'ORG'), ('10 million', 'CARDINAL'), ('500,000', 'CARDINAL'), ('June 30th', 'DATE'), ('China', 'GPE'), ('Hong Kong', 'GPE'), ('Hong Kong', 'GPE'), ('July', 'DATE'), ('July 1st', 'DATE'), ('Russia', 'GPE'), ('Putin', 'PERSON'), ('2036', 'DATE'), ('July 3rd', 'DATE'), ('COVID', 'ORG'), ('surpasses 11 million', 'CARDINAL'), ('July 7th', 'DATE'), ('Bulgaria', 'GPE'), ('93rd', 'ORDINAL'), ('Prosecutor Ivan Geshev', 'PERSON'), ('Thousands', 'CARDINAL'), ('Serbia', 'GPE'), ('Belgrade', 'GPE'), ('July 8th', 'DATE'), ('12 million', 'CARDINAL'), ('July 10th', 'DATE'), ('Turkey', 'GPE'), ('the Hagia Sophia', 'ORG'), ('Bulgaria', 'GPE'), ('Croatia', 'GPE'), ('ERM II', 'EVENT'), ('one', 'CARDINAL'), ('Euro', 'WORK_OF_ART'), ('July 13th', 'DATE'), ('Imihara', 'PRODUCT'), ('July 14th', 'DATE'), ('US', 'GPE'), ('the Hong Kong Autonomy Act', 'GPE'), ('China', 'GPE'), ('July 18th', 'DATE'), ('COVID-19', 'ORG'), ('600,000', 'CARDINAL'), ('July 21st', 'DATE'), ('European', 'NORP'), ('€750 billion', 'MONEY'), ('EU', 'ORG'), ('July 22nd', 'DATE'), ('COVID', 'ORG'), ('15 million', 'CARDINAL'), ('July 25th', 'DATE'), ('Kim Jong Un', 'PERSON'), ('Kaesong', 'PERSON'), ('COVID', 'ORG'), ('South Korea', 'GPE'), ('July 30th', 'DATE'), ('NASA', 'ORG'), ('Mars 2020', 'DATE'), ('August', 'DATE'), ('August', 'DATE'), ('4th', 'ORDINAL'), ('Two', 'CARDINAL'), ('Beirut', 'GPE'), ('Lebanon', 'GPE'), ('220', 'CARDINAL'), ('thousands', 'CARDINAL'), ('about 300,000', 'CARDINAL'), ('August 5th', 'DATE'), ('700,000', 'CARDINAL'), ('U.S.', 'GPE'), ('Health and Human Services', 'ORG'), ('Taiwan', 'GPE'), ('U.S.', 'GPE'), ('40 years', 'DATE'), ('August 9th', 'DATE'), ('Belarus', 'GPE'), ('Alexander Lukashenko', 'PERSON'), ('August 10th', 'DATE'), ('surpasses 20 million', 'CARDINAL'), ('August 11th', 'DATE'), ('Putin', 'PERSON'), ('Russia', 'GPE'), ('COVID-19', 'ORG'), ('August 13th', 'DATE'), ('Israel', 'GPE'), ('UAE', 'ORG'), ('August 14th', 'DATE'), ('Northern California', 'LOC'), ('August 18th', 'DATE'), ('Dale Hawerchuk', 'PERSON'), ('August 22nd', 'DATE'), ('800,000', 'CARDINAL'), ('August 25th', 'DATE'), ('Africa', 'LOC'), ('August 26th', 'DATE'), ('Jeff Bezos', 'PERSON'), ('first', 'ORDINAL'), ('$200 billion', 'MONEY'), ('August 27th', 'DATE'), ('Hurricane Laura', 'EVENT'), ('1856', 'DATE'), ('Last Island', 'EVENT'), ('Louisiana', 'GPE'), ('August 28th', 'DATE'), ('Chadwick Boseman', 'PERSON'), ('Japanese', 'NORP'), ('Shinzo Abe', 'PERSON'), ('August 30th', 'DATE'), ('COVID', 'ORG'), ('25 million', 'CARDINAL'), ('September', 'DATE'), ('September', 'DATE'), ('Israel', 'GPE'), ('Bahrain', 'GPE'), ('Kosovo', 'GPE'), ('Serbia', 'GPE'), ('September 5th', 'DATE'), ('September 10th', 'DATE'), ('August', 'DATE'), ('Complex Fire', 'LOC'), ('Californian', 'NORP'), ('second', 'ORDINAL'), ('2019-2020', 'DATE'), ('Columbian', 'NORP'), ('46 year old', 'DATE'), ('September 14th', 'DATE'), ('Phosphine', 'ORG'), ('Venus', 'LOC'), ('September 15th', 'DATE'), ('Israel', 'GPE'), ('UAE', 'ORG'), ('Bahrain', 'GPE'), ('#', 'CARDINAL'), ('2020', 'DATE'), ('5', 'CARDINAL'), ('Wikipedia', 'GPE'), ('Eurocentric', 'PERSON')]\n",
      "\n",
      "\n",
      "Title: AITA for telling my husband that I won't punish my daughter for speaking another language in the house because it upsets his son?\n",
      "Link: https://www.reddit.com/r/AmItheAsshole/comments/if3rmj/aita_for_telling_my_husband_that_i_wont_punish_my/\n",
      "Sentiment: Positive\n",
      "Entities: [('AITA', 'ORG'), ('Anya', 'PERSON'), ('India', 'GPE'), ('Hindi', 'GPE'), ('11 years old', 'DATE'), ('5', 'DATE'), ('60 year old', 'DATE'), ('Anya', 'PERSON'), ('6', 'DATE'), ('5 year old', 'DATE'), ('Ben', 'PERSON'), ('50%', 'PERCENT'), ('Anya', 'GPE'), ('Ben', 'PERSON'), ('Husband', 'PERSON'), ('Ben', 'PERSON'), ('Indian', 'NORP'), ('Anya', 'GPE'), ('Hindi', 'GPE'), ('Hindi', 'GPE'), ('English', 'LANGUAGE'), ('Few months ago', 'DATE'), ('Ben', 'PERSON'), ('Ben', 'PERSON'), ('A week ago', 'DATE'), ('Ben', 'PERSON'), ('Anya', 'PERSON'), ('Hindi', 'GPE'), ('Anya', 'PERSON'), ('Anya', 'GPE'), ('English', 'LANGUAGE'), ('Ben', 'PERSON'), ('Yesterday', 'DATE'), ('Hindi', 'GPE'), ('Anya', 'PERSON'), ('Ben', 'PERSON'), ('Later that night', 'TIME'), ('Hindi', 'GPE'), ('Ben', 'PERSON'), ('first', 'ORDINAL'), ('second', 'ORDINAL'), ('Ben', 'PERSON'), ('Indian', 'NORP'), ('Anya', 'GPE'), ('Anya', 'PERSON'), ('Ben', 'PERSON')]\n",
      "\n",
      "\n",
      "Title: Uncle bankrupts his previous employer\n",
      "Link: https://www.reddit.com/r/ProRevenge/comments/84gwk8/uncle_bankrupts_his_previous_employer/\n",
      "Sentiment: Positive\n",
      "Entities: [('Indian', 'NORP'), ('the 90s', 'DATE'), ('Australia', 'GPE'), ('3', 'CARDINAL'), ('Australia', 'GPE'), ('thousands of dollars', 'MONEY'), ('Indian', 'NORP'), ('Australian', 'NORP'), ('10+years', 'CARDINAL'), ('40 +', 'DATE'), ('GP', 'ORG'), ('GP', 'ORG'), ('Sunday', 'DATE'), ('GP', 'ORG'), ('Indian', 'NORP'), ('Saturday', 'DATE'), ('GP', 'ORG'), ('Indians', 'NORP'), ('Asians', 'NORP'), ('Aboriginal', 'ORG'), ('3 years', 'DATE'), ('GP', 'ORG'), ('The first year', 'DATE'), ('2 years', 'DATE'), ('GP', 'ORG'), ('4', 'CARDINAL'), ('2', 'CARDINAL'), ('2', 'CARDINAL'), ('2', 'CARDINAL'), ('GP', 'ORG'), ('2km', 'QUANTITY'), ('5 years', 'DATE'), ('6', 'CARDINAL'), ('4', 'CARDINAL'), ('6', 'CARDINAL'), ('1', 'CARDINAL'), ('3', 'CARDINAL'), ('3', 'CARDINAL'), ('one', 'CARDINAL'), ('3', 'CARDINAL')]\n",
      "\n",
      "\n",
      "Title: I spent the past year listening to over 7500 songs that were posted to reddit by independent artists. Have you ever wondered what the Grammy's would look like, if it only awarded undiscovered artists? Here are the 2022 Reddit Grammy Awards!\n",
      "Link: https://www.reddit.com/r/Music/comments/ul0yzq/i_spent_the_past_year_listening_to_over_7500/\n",
      "Sentiment: Positive\n",
      "Entities: [('the past year', 'DATE'), ('Grammy', 'PERSON'), ('2022', 'DATE'), ('This year', 'DATE'), ('182', 'CARDINAL'), ('179', 'CARDINAL'), ('Reddit', 'NORP'), ('2022', 'CARDINAL'), ('#', 'CARDINAL'), ('YouTube', 'ORG'), ('Apple Music', 'ORG'), ('XtZVbMt0Wq0', 'GPE'), ('#', 'CARDINAL'), ('The Chaw - Hours & Days](https://youtu.be/aREfvBU6WRo', 'WORK_OF_ART'), ('The Eves - Brand New', 'ORG'), ('Dream Of Sleeping - Anew](https://www.youtube.com', 'WORK_OF_ART'), ('Noah Bugalski - ca', 'PERSON'), ('noise](https://www.youtube.com', 'NORP'), ('NoahBugalski', 'PRODUCT'), ('EDM', 'ORG'), ('House', 'ORG'), ('Sertulariae - Phase', 'ORG'), ('Crèmium - COULER](https://www.youtube.com', 'ORG'), ('Oshua - Midnight Lows](https://www.youtube.com', 'WORK_OF_ART'), ('Washyb', 'PERSON'), ('Reggae', 'ORG'), ('Raynbird - Children of the Beast](https://www.youtube.com', 'WORK_OF_ART'), ('Macrowave - Dystopia](https://www.youtube.com', 'PERSON'), ('2', 'CARDINAL'), ('Felknia - Salty', 'ORG'), ('Vivid Fever Dreams - Get Through This', 'PERSON'), ('Mari Geti - Risky](https://www.youtube.com', 'PERSON'), ('Daniel Diaz - War', 'PERSON'), ('Bubba Bellin - Steel Guitar](https://youtu.be/KnjFge3Gls0', 'FAC'), ('Pingoin - Calm', 'PRODUCT'), ('Mistral - Burn](https://www.youtube.com', 'ORG'), ('Hop', 'PERSON'), ('C.T. Lee - A Sinister', 'PERSON'), ('Hop', 'PERSON'), ('Mike Fate - Natural](https://www.youtube.com', 'PERSON'), ('Chiel Nugter - Kwetsbaar](https://www.youtube.com', 'PERSON'), ('Wind](https://www.youtube.com', 'ORG'), ('Friends Of The Unknown - Recording Your Mind](https://soundcloud.com', 'WORK_OF_ART'), ('Lite - Apricot', 'PERSON'), ('Chill', 'ORG'), ('Latin Shui', 'NORP'), ('drive?](https://soundcloud.com', 'GPE'), ('Nacho Marques - The Sheriff](https://www.youtube.com/watch?v=', 'ORG'), ('Priscilla Hernandez - MORA-IA](https://youtu.be/4XLDqRsiH64', 'PERSON'), ('Synova - Jeal', 'ORG'), ('clickbate)](https://www.youtube.com', 'ORG'), ('Sky Loom - In the D](https://www.youtube.com/watch?v=2xMntLp0FcY', 'WORK_OF_ART'), ('Tochiro', 'GPE'), ('Scott', 'PERSON'), ('Summer', 'DATE'), ('Minev - Night Shift](https://youtu.be', 'WORK_OF_ART'), ('Best Alternative Pop', 'WORK_OF_ART'), ('Andrew M - Bounce](https://www.youtube.com', 'PERSON'), ('Catnip Cloud - Illumination](https://www.youtube.com', 'PERSON'), ('Vrdnyn', 'PERSON'), ('Summer School - Hurt', 'ORG'), ('Maeve - Manic Pixie Dream', 'ORG'), ('Daphne Cerez', 'PERSON'), ('Reuben Louis - The Secret Garden', 'PERSON'), ('jan-2021', 'DATE'), ('Mud Whale - Scapegoat](https://www.youtube.com', 'WORK_OF_ART'), ('Magic Jones - You', 'PRODUCT'), ('the Berry - Illusion', 'ORG'), ('Confusion](https://www.youtube.com', 'ORG'), ('LOHM', 'ORG'), ('David Petty - Dirt](https://open.spotify.com', 'PERSON'), ('Muma', 'PERSON'), ('Lloyd Degler - Sensible Oddities', 'ORG'), ('Flora Lin - Keep', 'PERSON'), ('Best Chillwave', 'GPE'), ('LeftWay', 'PERSON'), ('Goodwin Rainer - Paying', 'PERSON'), ('[Willie Dangerr', 'PERSON'), ('Daphne Cerez - Sacrifice', 'PERSON'), ('Best Psychedelic Folk:', 'WORK_OF_ART'), ('Noah Colley - I', 'PERSON'), ('Hop', 'PERSON'), ('to2yzV1iljU', 'GPE'), ('Zodat', 'GPE'), ('Tonze', 'GPE'), ('Marc Gedeon - Prelude', 'PERSON'), ('Best Alternative Trap', 'WORK_OF_ART'), ('Madach Ren - Olas](https://madachren.bandcamp.com', 'PERSON'), ('Hop', 'PERSON'), ('Max', 'PERSON'), ('Purple Gem', 'PERSON'), ('Evan Parisi-Sanchez', 'PERSON'), ('Louise Marshall - Blue](https://www.youtube.com', 'PERSON'), ('DHXP - Eyes Wide', 'ORG'), ('Bran - Breathe Slowly.](https://youtu.be/oDGBDcImCE4', 'WORK_OF_ART'), ('Sentientsimian', 'NORP'), ('Javii - Giants](https://www.youtube.com', 'PERSON'), ('House', 'ORG'), ('Chrylo - Vulcan', 'ORG'), ('Cola)](https://www.youtube.com', 'ORG'), ('qFidB-gguwQ', 'PERSON'), ('Hop', 'PERSON'), ('Hop', 'PERSON'), ('Forrest Del - Pain](https://www.youtube.com', 'PERSON'), ('VLN', 'ORG'), ('Retropxssy - Sleepy', 'ORG'), ('[Slick Naari - Why', 'PERSON'), ('Keyote - Grand', 'ORG'), ('Blind Lies', 'ORG'), ('The Opinion Industry -', 'WORK_OF_ART'), ('PIX3LARMY - Outside Your', 'PERSON'), ('Best Dreamgaze', 'WORK_OF_ART'), ('Mud Dog - Bite Down](https://www.youtube.com', 'WORK_OF_ART'), ('Thumb', 'PERSON'), ('the Love of God - Osay', 'WORK_OF_ART'), ('The F-use', 'PRODUCT'), ('LOHM](https://www.youtube.com', 'ORG'), ('Srabasti Acharya - Jao Pakhi](https://www.youtube.com/watch?v=dCRp1ml2rn4', 'PERSON'), ('PapaSon - Nada', 'ORG'), ('#', 'CARDINAL'), ('#', 'CARDINAL'), ('edit1', 'PERSON'), ('2022', 'CARDINAL'), ('3rd', 'ORDINAL'), ('Reddit Grammy post](https://www.reddit.com/r/', 'WORK_OF_ART'), ('Last year', 'DATE'), ('Reddit', 'NORP'), (\"next year's\", 'DATE'), ('Reddit Grammys', 'ORG'), (\"Reddit Grammy's\", 'PERSON'), ('#', 'CARDINAL')]\n",
      "\n",
      "\n",
      "Title: Indian clerk funds school fees of 45 underprivileged girls in daughters memory.\n",
      "Link: https://thelogicalindian.com/get-inspired/karnataka-clerk/\n",
      "Sentiment: Neutral\n",
      "Entities: [('Indian', 'NORP'), ('45', 'CARDINAL')]\n",
      "\n",
      "\n",
      "Title: 'We must kill the Indian in them'---Ryerson statue toppled in Toronto today--Ryerson was the founder of Canadian Indian School System where thousands of children died due to abuse, neglect, disease, poor sanitation and poor heating\n",
      "Link: https://imgur.com/SsV3prP\n",
      "Sentiment: Negative\n",
      "Entities: [('Indian', 'NORP'), ('Toronto', 'GPE'), ('today', 'DATE'), ('Ryerson', 'PERSON'), ('Canadian Indian School System', 'ORG'), ('thousands', 'CARDINAL')]\n",
      "\n",
      "\n",
      "Title: AITA for only feeding one child frozen food\n",
      "Link: https://www.reddit.com/r/AmItheAsshole/comments/v18xx5/aita_for_only_feeding_one_child_frozen_food/\n",
      "Sentiment: Positive\n",
      "Entities: [('AITA', 'ORG'), ('one', 'CARDINAL'), ('Indian', 'NORP'), ('9', 'CARDINAL'), ('10', 'CARDINAL'), ('Indian', 'NORP'), ('a little after 9', 'DATE')]\n",
      "\n",
      "\n",
      "Title: I’m still in the slammer for a long time but here’s an update\n",
      "Link: https://www.reddit.com/r/teenagers/comments/ijc4b4/im_still_in_the_slammer_for_a_long_time_but_heres/\n",
      "Sentiment: Positive\n",
      "Entities: [('two', 'CARDINAL'), ('first', 'ORDINAL'), ('Reddit', 'NORP'), ('Remindeme', 'PERSON'), ('half the day', 'DATE'), ('today', 'DATE'), ('Indian', 'NORP')]\n",
      "\n",
      "\n",
      "Title: Just a reminder that 160 years ago today, Mormons attacked, captured, and murdered at point-blank range an estimated 120 innocent pioneers traveling from Arkansas to California. Among the killed in the Mountain Meadows Massacre were 50 children.\n",
      "Link: https://www.reddit.com/r/atheism/comments/6t3m1b/just_a_reminder_that_160_years_ago_today_mormons/\n",
      "Sentiment: Negative\n",
      "Entities: [('160 years ago', 'DATE'), ('today', 'DATE'), ('Mormons', 'PERSON'), ('an estimated 120', 'CARDINAL'), ('Arkansas', 'GPE'), ('California', 'GPE'), ('50', 'CARDINAL'), ('the Mountain Meadows Massacre', 'ORG'), ('September* 11th', 'DATE'), ('August 11th', 'DATE'), ('160 years ago', 'DATE'), ('next* month', 'DATE'), (\"Jon Krakauer's\", 'PERSON'), ('the Banner of Heaven', 'ORG'), ('Fancher', 'PRODUCT'), ('only seventeen', 'CARDINAL'), ('no more than five years old', 'DATE'), ('Mormon', 'ORG'), ('Latter-day', 'ORG'), ('1859', 'DATE'), ('seventeen', 'CARDINAL'), ('Arkansas', 'GPE'), ('Mormon', 'ORG'), ('thousands of dollars', 'MONEY'), ('Mormons', 'PERSON'), ('Southern Paiute Indian', 'LOC'), ('Paiute', 'ORG'), ('Mormon', 'ORG'), ('Mormon', 'NORP'), ('John D.]', 'PERSON'), ('days', 'DATE'), ('Lee', 'PERSON'), ('Mormons', 'PERSON'), ('Indians', 'NORP'), ('Mormons', 'PERSON')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text (tokenization, lemmatization, stopword removal)\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Tokenize, lemmatize, remove stopwords and non-alphabetic tokens\n",
    "    tokens = [token.lemma_ for token in doc if token.text.isalpha() and token.text.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to extract entities and sentiment\n",
    "def extract_info(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]  # Extract named entities\n",
    "    sentiment = TextBlob(text).sentiment.polarity  # Sentiment analysis (-1 to 1)\n",
    "    return entities, sentiment\n",
    "\n",
    "# Function to summarize events from posts\n",
    "def summarize_events(posts):\n",
    "    summaries = []\n",
    "    for post in posts:\n",
    "        title = post.get('Title', '')\n",
    "        body = post.get('Body/Content', '')\n",
    "        link = post.get('Link', '')\n",
    "        \n",
    "        # Preprocess the text\n",
    "        full_text = f\"{title} {body}\"\n",
    "        preprocessed_text = preprocess_text(full_text)\n",
    "        \n",
    "        # Extract entities and sentiment\n",
    "        entities, sentiment = extract_info(full_text)\n",
    "        \n",
    "        # Create a summary dictionary\n",
    "        summary = {\n",
    "            'Title': title,\n",
    "            'Link': link,\n",
    "            'Sentiment': sentiment,\n",
    "            'Entities': entities\n",
    "        }\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Load JSON data from file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Main function to process the file and display summaries\n",
    "def main():\n",
    "    file_path = r'C:\\Users\\yashswi shukla\\Desktop\\Narrative\\Code\\reddit_top_posts_indian school.json'  # Replace with your file path\n",
    "    posts = load_json(file_path)\n",
    "    \n",
    "    # Summarize events\n",
    "    summaries = summarize_events(posts)\n",
    "    \n",
    "    # Display summaries\n",
    "    for summary in summaries:\n",
    "        print(f\"Title: {summary['Title']}\")\n",
    "        print(f\"Link: {summary['Link']}\")\n",
    "        print(f\"Sentiment: {'Positive' if summary['Sentiment'] > 0 else 'Negative' if summary['Sentiment'] < 0 else 'Neutral'}\")\n",
    "        print(f\"Entities: {summary['Entities']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\matutils.py\", line 1356, in <module>\n",
      "    from gensim.corpora._mmreader import MmReader  # noqa: F401\n",
      "  File \"gensim\\corpora\\_mmreader.pyx\", line 11, in init gensim.corpora._mmreader\n",
      "ImportError: cannot import name utils\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\yashswi shukla\\AppData\\Local\\Temp\\ipykernel_21740\\923213520.py\", line 7, in <cell line: 7>\n",
      "    from gensim.summarization import summarize  # For overall summarization\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\__init__.py\", line 11, in <module>\n",
      "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\corpora\\__init__.py\", line 6, in <module>\n",
      "    from .indexedcorpus import IndexedCorpus  # noqa:F401 must appear before the other classes\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py\", line 14, in <module>\n",
      "    from gensim import interfaces, utils\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\interfaces.py\", line 19, in <module>\n",
      "    from gensim import utils, matutils\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\gensim\\matutils.py\", line 1358, in <module>\n",
      "    raise utils.NO_CYTHON\n",
      "RuntimeError: Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "from gensim.summarization import summarize  # For overall summarization\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text (tokenization, lemmatization, stopword removal)\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Tokenize, lemmatize, remove stopwords and non-alphabetic tokens\n",
    "    tokens = [token.lemma_ for token in doc if token.text.isalpha() and token.text.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to extract entities and sentiment\n",
    "def extract_info(text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)  # Combine list of strings into a single string\n",
    "    elif not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string or a list of strings.\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]  # Extract named entities\n",
    "    sentiment = TextBlob(text).sentiment.polarity  # Sentiment analysis (-1 to 1)\n",
    "    return entities, sentiment\n",
    "\n",
    "# Function to summarize events from posts\n",
    "def summarize_events(posts):\n",
    "    summaries = []\n",
    "    combined_text = \"\"  # For overall summarization\n",
    "    for post in posts:\n",
    "        title = post.get('Title', '')\n",
    "        body = post.get('Body/Content', '')\n",
    "        link = post.get('Link', '')\n",
    "        \n",
    "        # Preprocess the text\n",
    "        full_text = f\"{title} {body}\"\n",
    "        preprocessed_text = preprocess_text(full_text)\n",
    "        \n",
    "        # Extract entities and sentiment\n",
    "        entities, sentiment = extract_info(full_text)\n",
    "        \n",
    "        # Create a summary dictionary\n",
    "        summary = {\n",
    "            'Title': title,\n",
    "            'Link': link,\n",
    "            'Sentiment': sentiment,\n",
    "            'Entities': entities\n",
    "        }\n",
    "        summaries.append(summary)\n",
    "        \n",
    "        # Combine text for overall summary\n",
    "        combined_text += full_text + ' '  # Add to combined text\n",
    "    \n",
    "    return summaries, combined_text\n",
    "\n",
    "# Load JSON data from file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Function to generate overall summary\n",
    "def generate_overall_summary(combined_text):\n",
    "    try:\n",
    "        overall_summary = summarize(combined_text, ratio=0.1)  # Adjust ratio for summary length\n",
    "    except ValueError:\n",
    "        overall_summary = \"Summary could not be generated due to insufficient text length.\"\n",
    "    return overall_summary\n",
    "\n",
    "# Main function to process the file and display summaries\n",
    "def main():\n",
    "    file_path = '/mnt/data/reddit_top_posts_indian school.json'  # Replace with your file path\n",
    "    posts = load_json(file_path)\n",
    "    \n",
    "    # Summarize events\n",
    "    summaries, combined_text = summarize_events(posts)\n",
    "    \n",
    "    # Generate overall summary\n",
    "    overall_summary = generate_overall_summary(combined_text)\n",
    "    \n",
    "    # Display individual summaries\n",
    "    for summary in summaries:\n",
    "        print(f\"Title: {summary['Title']}\")\n",
    "        print(f\"Link: {summary['Link']}\")\n",
    "        print(f\"Sentiment: {'Positive' if summary['Sentiment'] > 0 else 'Negative' if summary['Sentiment'] < 0 else 'Neutral'}\")\n",
    "        print(f\"Entities: {summary['Entities']}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # Display overall summary\n",
    "    print(\"Overall Summary:\")\n",
    "    print(overall_summary)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
