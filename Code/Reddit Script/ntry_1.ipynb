{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\yashswi\n",
      "[nltk_data]     shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\yashswi\n",
      "[nltk_data]     shukla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "trump commonly refers donald trump born united states trump card games playing card given high rank trump may also refer trump dog pug owned english painter william hogarth trump horse australian racehorse donald trump foundation charity trump organization business conglomerate founded trump shuttle airline callsign trump trump surname including list people fictional characters name trump gamer jeffrey shih born hearthstone trump colorado trump maryland trump islands antarctica trump islands newfoundland labrador canada trump street city london\n",
      "\n",
      "=== Keywords ===\n",
      "born, card, donald, islands, trump\n",
      "\n",
      "=== Source URL ===\n",
      "https://en.wikipedia.org/wiki/Trump\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to extract keywords using TF-IDF\n",
    "def extract_keywords(text, top_n=5):\n",
    "    vectorizer = TfidfVectorizer(max_features=top_n, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    keywords = vectorizer.get_feature_names_out()\n",
    "    return keywords\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, sentence_count=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_frequencies = Counter(word_tokenize(text.lower()))\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] /= max_frequency\n",
    "    sentence_scores = {\n",
    "        sentence: sum(word_frequencies.get(word, 0) for word in word_tokenize(sentence.lower()))\n",
    "        for sentence in sentences\n",
    "    }\n",
    "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "    summary = ' '.join(sorted_sentences[:sentence_count])\n",
    "    return summary\n",
    "\n",
    "# Function to process JSON and generate summary\n",
    "def process_json_and_summarize(json_file, sentence_count=3, keyword_count=5):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combine all content from sections and summary\n",
    "    text_content = data.get('Summary', '') + ' '\n",
    "    for section in data.get('Sections', []):\n",
    "        text_content += section.get('Content', '') + ' '\n",
    "\n",
    "    # Preprocess text\n",
    "    cleaned_text = preprocess_text(text_content)\n",
    "\n",
    "    # Generate summary\n",
    "    summary = summarize_text(cleaned_text, sentence_count)\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = extract_keywords(cleaned_text, keyword_count)\n",
    "\n",
    "    result = {\n",
    "        \"Title\": data.get('Title', 'Unknown'),\n",
    "        \"URL\": data.get('URL', 'Unknown'),\n",
    "        \"Summary\": summary,\n",
    "        \"Keywords\": keywords\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Input JSON file\n",
    "    json_file = input(\"Enter the path to the Wikipedia JSON file: \").strip()\n",
    "\n",
    "    # Generate summary and keywords\n",
    "    result = process_json_and_summarize(json_file)\n",
    "\n",
    "    # Output the result\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(result[\"Summary\"])\n",
    "    print(\"\\n=== Keywords ===\")\n",
    "    print(\", \".join(result[\"Keywords\"]))\n",
    "    print(\"\\n=== Source URL ===\")\n",
    "    print(result[\"URL\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to preprocess and summarize JSON content using transformers\n",
    "def summarize_with_transformer(json_file, max_summary_length=1500):\n",
    "    # Load the JSON file\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combine all content from the summary and sections\n",
    "    text_content = data.get('Summary', '') + ' '\n",
    "    for section in data.get('Sections', []):\n",
    "        text_content += section.get('Content', '') + ' '\n",
    "\n",
    "    # Initialize the summarization pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "    # Generate a summary\n",
    "    summary = summarizer(text_content, max_length=max_summary_length, min_length=30, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    result = {\n",
    "        \"Title\": data.get('Title', 'Unknown'),\n",
    "        \"URL\": data.get('URL', 'Unknown'),\n",
    "        \"Summary\": summary\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Input JSON file\n",
    "    json_file = input(\"Enter the path to the Wikipedia JSON file: \").strip()\n",
    "\n",
    "    # Summarize using the transformer model\n",
    "    try:\n",
    "        result = summarize_with_transformer(json_file)\n",
    "\n",
    "        # Output the results\n",
    "        print(\"\\n=== Title ===\")\n",
    "        print(result[\"Title\"])\n",
    "        print(\"\\n=== Summary ===\")\n",
    "        print(result[\"Summary\"])\n",
    "        print(\"\\n=== Source URL ===\")\n",
    "        print(result[\"URL\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Sample usage with the large India text\u001b[39;00m\n\u001b[0;32m     25\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mlarge India text from the user request}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_large_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36msummarize_large_text\u001b[1;34m(text, model, max_chunk_length)\u001b[0m\n\u001b[0;32m      8\u001b[0m summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[1;32m---> 11\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSummarize the following text:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchunk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Combine all summaries into a final summary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yashswi shukla\\anaconda3\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Function to summarize large text\n",
    "def summarize_large_text(text, model=\"gpt-3.5-turbo\", max_chunk_length=1000):\n",
    "    # Split text into smaller chunks if it's too long\n",
    "    text_chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in text_chunks:\n",
    "        response = openai.Completion.create(\n",
    "            model=model,\n",
    "            prompt=f\"Summarize the following text:\\n\\n{chunk}\",\n",
    "            temperature=0.5,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        summaries.append(response.choices[0].text.strip())\n",
    "    \n",
    "    # Combine all summaries into a final summary\n",
    "    final_summary = \"\\n\".join(summaries)\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "# Sample usage with the large India text\n",
    "text = \"\"\"{large India text from the user request}\"\"\"\n",
    "summary = summarize_large_text(text)\n",
    "\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
